# Configuration

Configure Copinance OS for your needs.

## Configuration Methods

Copinance OS supports two ways to configure LLM settings:

1. **CLI Usage**: Environment variables (for command-line usage)
2. **Library Integration**: `LLMConfig` object (for programmatic usage)

> **Note for Library Integrators**: If you're integrating Copinance OS into your application, you should provide `LLMConfig` directly instead of relying on environment variables. See [Library Integration](#library-integration) below.

## Environment Variables (CLI Usage)

For CLI usage, configuration is done through environment variables or a `.env` file.

### Creating .env File

Create a `.env` file in your project root (same directory as `pyproject.toml`):

```bash
# .env file
COPINANCEOS_GEMINI_API_KEY=your-api-key-here
```

## LLM Provider Setup

Copinance OS supports multiple LLM providers for agentic workflows. You can use either Gemini (cloud-based) or Ollama (local).

### Gemini API (Cloud-based)

Recommended for most users. Requires an API key.

### 1. Get API Key

1. Go to [Google AI Studio](https://aistudio.google.com)
2. Sign in with your Google account
3. Click "Create API Key"
4. Copy your API key

### 2. Configure

**Option A: .env file (Recommended)**
```bash
COPINANCEOS_GEMINI_API_KEY=your-api-key-here
```

**Option B: Environment Variable**
```bash
export COPINANCEOS_GEMINI_API_KEY=your-api-key-here
```

### 3. Verify

```bash
copinance research ask "What is the current price?" --symbol AAPL
```

If configured correctly, you'll see AI analysis. If not, check:
- `.env` file is in the project root
- Variable name is exactly `COPINANCEOS_GEMINI_API_KEY`
- No extra spaces around the `=` sign

## Model Selection

Choose which Gemini model to use:

```bash
# Default (most capable)
COPINANCEOS_GEMINI_MODEL=gemini-1.5-pro

# Latest and fastest
COPINANCEOS_GEMINI_MODEL=gemini-2.5-flash

# Faster alternative
COPINANCEOS_GEMINI_MODEL=gemini-1.5-flash
```

**Default:** `gemini-1.5-pro` (most capable). Use `gemini-2.5-flash` for faster responses.

## Storage Configuration

Configure where data is stored:

```bash
# Storage type: 'memory' or 'file'
COPINANCEOS_STORAGE_TYPE=file

# Storage path (for file storage)
COPINANCEOS_STORAGE_PATH=~/.copinanceos
```

## Complete .env Example

**Using Gemini (Cloud):**
```bash
# LLM Provider (optional, default: gemini)
COPINANCEOS_LLM_PROVIDER=gemini

# Gemini API (required for agentic workflows with Gemini)
COPINANCEOS_GEMINI_API_KEY=AIzaSy...your-actual-key

# Model selection (optional, default: gemini-1.5-pro)
COPINANCEOS_GEMINI_MODEL=gemini-1.5-pro

# Storage (optional)
COPINANCEOS_STORAGE_TYPE=file
COPINANCEOS_STORAGE_PATH=~/.copinanceos
```

**Using Ollama (Local):**
```bash
# LLM Provider
COPINANCEOS_LLM_PROVIDER=ollama

# Ollama configuration (optional, defaults shown)
COPINANCEOS_OLLAMA_BASE_URL=http://localhost:11434
COPINANCEOS_OLLAMA_MODEL=llama2

# Storage (optional)
COPINANCEOS_STORAGE_TYPE=file
COPINANCEOS_STORAGE_PATH=~/.copinanceos
```

**Mixed Configuration (per-workflow):**
```bash
# Use different providers for different workflows
COPINANCEOS_WORKFLOW_LLM_PROVIDERS=static:ollama,agentic:gemini

# Gemini API (for agentic workflows)
COPINANCEOS_GEMINI_API_KEY=AIzaSy...your-actual-key
COPINANCEOS_GEMINI_MODEL=gemini-1.5-pro

# Ollama (for static workflows)
COPINANCEOS_OLLAMA_BASE_URL=http://localhost:11434
COPINANCEOS_OLLAMA_MODEL=llama2
```

## Security

⚠️ **Never commit your `.env` file to version control!**

Make sure `.env` is in your `.gitignore`:
```gitignore
.env
.env.local
```

## Troubleshooting

### "LLM analyzer not configured"
- Check `.env` file location (should be in project root)
- Verify variable name is correct
- Restart terminal after setting environment variables

### "Gemini not available"
Install the required package:
```bash
pip install google-genai
```

### "Gemini API key is not configured"
- Verify API key is set correctly
- Check for typos in variable name
- Ensure no quotes around the key value

## Ollama Setup (Local LLM)

Use Ollama to run LLMs locally without API keys. Great for privacy and cost savings.

### 1. Install Ollama

1. Download from [ollama.ai](https://ollama.ai)
2. Install and start the Ollama service
3. Pull a model: `ollama pull llama2` (or `mistral`, `codellama`, etc.)

### 2. Configure

**Option A: .env file (Recommended)**
```bash
# Set Ollama as the default provider
COPINANCEOS_LLM_PROVIDER=ollama

# Optional: Customize Ollama settings
COPINANCEOS_OLLAMA_BASE_URL=http://localhost:11434
COPINANCEOS_OLLAMA_MODEL=llama2
```

**Option B: Environment Variable**
```bash
export COPINANCEOS_LLM_PROVIDER=ollama
export COPINANCEOS_OLLAMA_MODEL=llama2
```

### 3. Verify

```bash
copinance research ask "What is the current price?" --symbol AAPL
```

### Per-Workflow Provider Configuration

You can use different providers for different workflows:

```bash
# Use Ollama for static workflows, Gemini for agentic
COPINANCEOS_WORKFLOW_LLM_PROVIDERS=static:ollama,agentic:gemini
```

This allows you to use local models for simple tasks and cloud models for complex analysis.

## Library Integration

If you're integrating Copinance OS into your application as a library, you should provide LLM configuration using the `LLMConfig` class instead of environment variables.

### Using LLMConfig

```python
from copinanceos.infrastructure.analyzers.llm.config import LLMConfig
from copinanceos.infrastructure.containers import get_container

# Create LLM configuration
llm_config = LLMConfig(
    provider="gemini",
    api_key="your-api-key",
    model="gemini-1.5-pro",
    temperature=0.7,
)

# Create container with LLM config
container = get_container(llm_config=llm_config)

# Use the container
use_case = container.get_stock_use_case()
# ... use the use case
```

### Per-Workflow Provider Configuration

You can configure different providers for different workflows:

```python
llm_config = LLMConfig(
    provider="gemini",  # Default provider
    api_key="your-gemini-key",
    workflow_providers={
        "static": "ollama",      # Use Ollama for static workflows
        "agentic": "gemini",     # Use Gemini for agentic workflows
    },
)
```

### Direct Provider Creation

You can also create providers directly:

```python
from copinanceos.infrastructure.analyzers.llm.config import LLMConfig
from copinanceos.infrastructure.analyzers.llm.providers.factory import LLMProviderFactory
from copinanceos.infrastructure.factories.llm_analyzer import LLMAnalyzerFactory

# Create LLM config
llm_config = LLMConfig(
    provider="gemini",
    api_key="your-api-key",
    model="gemini-1.5-pro",
)

# Create provider
provider = LLMProviderFactory.create_provider("gemini", llm_config=llm_config)

# Create analyzer
analyzer = LLMAnalyzerFactory.create("gemini", llm_config=llm_config)
```

### Why Use LLMConfig?

- **Security**: API keys are not stored in environment variables
- **Flexibility**: Different configurations for different parts of your application
- **Testability**: Easy to mock and test with different configurations
- **Type Safety**: Full type checking support with dataclasses

